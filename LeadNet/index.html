<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0036)https://chuxwa.github.io/SE-ORNet/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="keywords" content="LeadNet">
  <meta name="description" content="LeadNet">
  <link href="./main.css" media="all" rel="stylesheet">
  <link rel="icon" type="image/png" href="https://chuxwa.github.io/LeadNet/">
  <title>Long-short Range Adaptive Transformer with Dynamic Sampling for 3D Object Detection</title>
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./css2" rel="stylesheet">
  <script type="text/javascript" src="./jquery-1.12.4.min.js"></script>
</head>

<!-- cover -->

<body>
  <h3 align="center">TCSVT 2023</h3>
  <h1 align="center">Long-short Range Adaptive Transformer with Dynamic Sampling for 3D Object Detection</h1>

  <p align="center" style="font-size:18px">
    <a href="https://chuxwa.github.io/"><span>ChuXin Wang</span></a><sup>1</sup>* 
    <a href="https://github.com/JiachengDeng"><span>Jiacheng Deng</span></a><sup>1</sup>* 
    <a href="https://github.com/Hevans123"><span>Jianfeng He</span></a><sup>1</sup> 
    <a href="http://staff.ustc.edu.cn/~tzzhang/"><span>Tianzhu Zhang</span></a><sup>1,3</sup> 
    <span>Zhe Zhang</span></a><sup>3</sup>
    <span>Yongdong Zhang</span></a><sup>1</sup> 
  </p>
  <p align="center" style="font-size:18px">
    <sup>1</sup><a href="http://en.ustc.edu.cn/">University of Science and Technology of China</a>   <sup>2</sup><a
      href="https://www.cast.cn/english">China Academy of Space Technology</a>  
    <br><br> <sup>3</sup><a href="https://special.zhaopin.com/Flying/pagepublish/140408317/index.html#">Deep Space
      Exploration Lab</a>
  </p>
  <p>

  </p>
  <p align="center">
    *Equal contribution
  </p>


  <p align="center">
    <a href="https://ieeexplore.ieee.org/abstract/document/10124821">[Paper]</a>
    <a href="https://github.com/OpenSpaceAI/LeadNet">[Code]</a>
    <a href="https://chuxwa.github.io/LeadNet/">[Website]</a>
    <a href="https://chuxwa.github.io/LeadNet/files/bib.txt">[BibTeX]</a>
  </p>
  <h2>Abstract</h2>
  <hr>

  <p>
    3D object detection in point cloud aims at simultaneously localizing and recognizing 3D objects from a 3D point set. However, since point clouds are usually sparse, unordered, and irregular, it is challenging to learn robust point representations and sample high-quality object queries. To deal with the above issues, we propose a Long-short rangE Adaptive transformer with Dynamic sampling (LeadNet), including a point representation encoder, a dynamic object query sampling decoder, and an object detection decoder in a unified architecture for 3D object detection. Specifically, in the point representation encoder, we combine an attention layer and a channel attentive kernel convolution layer to consider the local structure and the long-range context simultaneously. In the dynamic object query sampling decoder, we utilize multiple dynamic prototypes to adapt to various point clouds. In the object detection decoder, we incorporate a dynamic Gaussian weight map into the cross-attention mechanism to help the detection decoder focus on the proper visual regions near the object, further accelerating the training process. Extensive experimental results on two standard benchmarks show that our LeadNet outperforms the 3DETR baseline by 11.6% mAP50 on the ScanNet v2 dataset and achieves the new state-of-the-art results on ScanNet v2 and SUN RGB-D benchmarks for the geometric-only approaches.
  </p>
  <br>

  <h2>Overview</h2>
  <hr>
  <p align="center">
    <img src="./figures/Pipeline_00.png" width="850" alt="centered image">
  </p>
  <p>
    The architecture of our LeadNet consists of three major components, including the Point Representation Encoder, the Object Query Sampling Decoder, and the Object Detection Decoder.The downsampling point cloud is first obtained through the frequently-used SA module.Then the point representation encoder, composed of an attention branch and a channel-wise kernel convolution branch, extracts point representations from the downsampling point cloud.Next, and based on these point representations, the object query sampling decoder samples a certain number of qualified object queries.Finally, in the object detection decoder, these object queries aggregate features across the entire scene with the cross-attention mechanism for the subsequent object detection. For more details, please refer to the text.
  </p>
  <br>

  <h2>LeadNet</h2>
  <hr>
  <br><br><table id="tbInformation" width="100%">
      <tbody>
          <tr>
              <td rowspan="3">
                  <div class="smallimg"> 
                      <img src="./figures/encoder_new_00.png" width="450" class="left">
                  </div>
                  
              </td>
        
              <td>
                 
                  <div class="smallpara">
                      <p> 
                        The structure of the point representation encoder. In the point representation encoder, we introduce the channel-wise kernel convolution into the self-attention mechanism to efficiently extract the local structure feature.
                      </p>
                   </div>
              </td>
          </tr>
      </tbody>
      

      <tbody>
          <tr>
             
              <td>
                  <div class="smallpara">
                      <p> 
                        The structure of object query sampling decoder. We introduce a set of learnable prototype query filters that can dynamically adapt to the specific scene. And we calculate the cosine similarity between the points and the prototypes to sample a certain number of object queries.
                      </p>
                   </div>
              </td>
              <td rowspan="3">
                  <div class="smallimg"> 
                      <img src="./figures/querydecoder_00.png" width="450" class="left">
                  </div>
                 
              </td>
          </tr>
      </tbody>
  </table>

  <br>
  <h2>Results</h2>
  <hr>

  <p align="center">
    <span>
      Comparison on the ScanNet V2 validation set. We show per-category results of average precision (AP) with a 3D IoU threshold of 0.25 and a mean of AP across all semantic classes. L denotes the decoder depth. Since the per-category results and checkpoints of the training model are not available, we only show the mean of AP across all semantic classes in T3D.
    </span>
    <img src="./figures/LeadNet_06_1.png" width="850" class="center">
  </p>

  <p align="center">
    <span>
      Comparison on the ScanNet V2 validation set. We show per-category results of average precision (AP) with a 3D IoU threshold of 0.50 and a mean of AP across all semantic classes. L denotes the decoder depth.
    </span>
    <img src="./figures/LeadNet_06_2.png" width="850" class="center">
  </p>

  <p align="center">
    <span>
      Comparison on the SUN RGB-D validation set. We show per-category results of average precision (AP) with a 3D IoU threshold of 0.25 and a mean of AP across all semantic classes. L denotes the decoder depth. Since the per-category results and checkpoints of the training model are not available, we only show the mean of AP across all semantic classes in T3D.
    </span>
    <img src="./figures/LeadNet_06_3.png" width="700" class="center">
  </p>

  <p align="center">
    <span>
      Comparison on the SUN RGB-D validation set. We show per-category results of average precision (AP) with a 3D IoU threshold of 0.50 and a mean of AP across all semantic classes. L denotes the decoder depth.
    </span>
    <img src="./figures/LeadNet_07.png" width="700" class="center">
  </p>
  <br>


  <h2>Paper</h2>
  <hr>


  <table id="tbInformation" width="100%">
    <tbody>
      <tr>
        <td rowspan="3">
          <div class="shadow">
            <img src="./figures/firstpage.png" width="170">
          </div>

        </td>
        <td>
          <div class="paper">
            <p>
              <b>Long-short Range Adaptive Transformer with Dynamic Sampling for 3D Object Detection</b>
              <br>
              ChuXin Wang*, Jiacheng Deng*, Jianfeng He, Tianzhu Zhang, Zhe Zhang, Yongdong Zhang
              <br>
              IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023
              <br>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10124821">[PDF]</a>
              <a href="https://github.com/OpenSpaceAI/LeadNet">[Code]</a>
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Xie_Style-Based_Point_Generator_With_Adversarial_Rendering_for_Point_Cloud_Completion_CVPR_2021_paper.html">[CVPR 2021 Open Access]</a>  -->
              <a href="https://chuxwa.github.io/LeadNet/">[Website]</a>
              <!-- <a href="https://chuxwa.github.io/LeadNet/files/cvpr21_poster_.pdf">[Poster]</a> -->
              <a href="https://chuxwa.github.io/LeadNet/files/bib.txt">[BibTeX]</a>
            </p>
          </div>
        </td>
      </tr>
    </tbody>
  </table>

  <br>

  <p align="center">
    <font color="#999999">Last update: Mar, 2021</font>
  </p>

  <div class="jvectormap-tip"></div>

</body>

</html>